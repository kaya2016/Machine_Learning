{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os # to open system file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_built</th>\n",
       "      <th>stories</th>\n",
       "      <th>num_bedrooms</th>\n",
       "      <th>full_bathrooms</th>\n",
       "      <th>half_bathrooms</th>\n",
       "      <th>livable_sqft</th>\n",
       "      <th>total_sqft</th>\n",
       "      <th>garage_type</th>\n",
       "      <th>garage_sqft</th>\n",
       "      <th>carport_sqft</th>\n",
       "      <th>has_fireplace</th>\n",
       "      <th>has_pool</th>\n",
       "      <th>has_central_heating</th>\n",
       "      <th>has_central_cooling</th>\n",
       "      <th>house_number</th>\n",
       "      <th>street_name</th>\n",
       "      <th>unit_number</th>\n",
       "      <th>city</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>sale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1978</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1689</td>\n",
       "      <td>1859</td>\n",
       "      <td>attached</td>\n",
       "      <td>508</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>42670</td>\n",
       "      <td>Lopez Crossing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hallfort</td>\n",
       "      <td>10907</td>\n",
       "      <td>270897.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1958</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1984</td>\n",
       "      <td>2002</td>\n",
       "      <td>attached</td>\n",
       "      <td>462</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5194</td>\n",
       "      <td>Gardner Park</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hallfort</td>\n",
       "      <td>10907</td>\n",
       "      <td>302404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1581</td>\n",
       "      <td>1578</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>625</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4366</td>\n",
       "      <td>Harding Islands</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lake Christinaport</td>\n",
       "      <td>11203</td>\n",
       "      <td>2519996.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1829</td>\n",
       "      <td>2277</td>\n",
       "      <td>attached</td>\n",
       "      <td>479</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3302</td>\n",
       "      <td>Michelle Highway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lake Christinaport</td>\n",
       "      <td>11203</td>\n",
       "      <td>197193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1580</td>\n",
       "      <td>1749</td>\n",
       "      <td>attached</td>\n",
       "      <td>430</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>582</td>\n",
       "      <td>Jacob Cape</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lake Christinaport</td>\n",
       "      <td>11203</td>\n",
       "      <td>207897.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_built  stories  num_bedrooms  full_bathrooms  half_bathrooms  \\\n",
       "0        1978        1             4               1               1   \n",
       "1        1958        1             3               1               1   \n",
       "2        2002        1             3               2               0   \n",
       "3        2004        1             4               2               0   \n",
       "4        2006        1             4               2               0   \n",
       "\n",
       "   livable_sqft  total_sqft garage_type  garage_sqft  carport_sqft  \\\n",
       "0          1689        1859    attached          508             0   \n",
       "1          1984        2002    attached          462             0   \n",
       "2          1581        1578        none            0           625   \n",
       "3          1829        2277    attached          479             0   \n",
       "4          1580        1749    attached          430             0   \n",
       "\n",
       "   has_fireplace  has_pool  has_central_heating  has_central_cooling  \\\n",
       "0           True     False                 True                 True   \n",
       "1           True     False                 True                 True   \n",
       "2          False     False                 True                 True   \n",
       "3           True     False                 True                 True   \n",
       "4           True     False                 True                 True   \n",
       "\n",
       "   house_number       street_name  unit_number                city  zip_code  \\\n",
       "0         42670    Lopez Crossing          NaN            Hallfort     10907   \n",
       "1          5194      Gardner Park          NaN            Hallfort     10907   \n",
       "2          4366   Harding Islands          NaN  Lake Christinaport     11203   \n",
       "3          3302  Michelle Highway          NaN  Lake Christinaport     11203   \n",
       "4           582        Jacob Cape          NaN  Lake Christinaport     11203   \n",
       "\n",
       "   sale_price  \n",
       "0    270897.0  \n",
       "1    302404.0  \n",
       "2   2519996.0  \n",
       "3    197193.0  \n",
       "4    207897.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset into a data table using Pandas\n",
    "df = pd.read_csv(\"ml_house_data_set.csv\")\n",
    "\n",
    "# Create a web page view of the data for easy viewing\n",
    "html = df[0:100].to_html()\n",
    "\n",
    "# Save the html to a temporary file then we can open it using the web browser\n",
    "with open(\"data.html\", \"w\") as f:\n",
    "    f.write(html)\n",
    "\n",
    "# Open the web page in our web browser\n",
    "full_filename = os.path.abspath(\"data.html\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Engineering\n",
    "We can see that the featuer *garage_type* take 3 possible values\n",
    "* none: no carage\n",
    "* attatechd: the carage attached to the house\n",
    "* detached: the carage in seprate building\n",
    "\n",
    "we need to preporss the data by applying one hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the hause address: the house number the street name the znit number the coty and the zip code. The house number and the unit number  aren't useful, therefor we want to drope them from our model. \n",
    "\n",
    "IF we know the zip code of the house then we know the city of the house, therefor we need only to include one of them. The name of the street may not influnece the price of the house but it will make our model more complex. If we apply the one hot encoding then we will end up with a new feature for every single street."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The house sell price would be out y value which we are trying to predict with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the fields from the data set that we don't want to include in our model\n",
    "del df['house_number']\n",
    "del df['unit_number']\n",
    "del df['street_name']\n",
    "del df['zip_code']\n",
    "\n",
    "# Replace categorical data with one-hot encoded data\n",
    "features_df = pd.get_dummies(df, columns= ['garage_type','city'] )\n",
    "\n",
    "# Remove the sale price from the feature data\n",
    "del features_df['sale_price']\n",
    "\n",
    "# Create the X and y arrays\n",
    "X = features_df.as_matrix()   # we want x to be a NumPY matrix data type and not a pandas dataframe\n",
    "y = df['sale_price'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Curse of Dimesionality: ** As the number of dimensions (or features) in the data increases, the number of data points required to build a good model growes exponentially.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do two things:\n",
    "* shuffle the data\n",
    "* split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "# Split the data set in a training set (70%) and a test set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and train our model. We use an algorithm called gradient boosting and then we will set the hyper-parameters for this model.\n",
    "* **n_estimators**: tells the model how many decision trees to build. Higher number allow the model to be more accurate but it increases the amount of time required to run the model.' \n",
    "* **learning_rate**:  controls how much each additional decision tree influences the overall predicition. Lower rates usually lead to higher accuracy. \n",
    "* **max_depth**: controls how many layers deep each indivialual decision tree cann be.\n",
    "* **min_samples_leaf**: controls how many times a value must appear in our training set for decision tree to make a decision based on it. For example if we say 9 then that means at least 9 houses must exhibit the same characterisitic before we considet it meaningful enough to build decision tree around it. This helps us to prevent the influence of outlier on our model.  \n",
    "* **max_feature**: is the percentage of features in our model that we randomly choose to consider to create a branche in our decision tree.\n",
    "* **loss**: controls how scikit-learn calculates the model's error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trained_house_classifier_model.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Fit regression model\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=9,\n",
    "    max_features=0.1,\n",
    "    loss='huber',\n",
    "    random_state=0\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model to a file so we can use it in other programs\n",
    "joblib.dump(model, 'trained_house_classifier_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next stip is to measure how well the model performs. We will use a measurment called **Mean Absolute Error** which looks at every pridiction and it gives us an average of how wrong it was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Mean Absolute Error: 48331.8249\n",
      "Test Set Mean Absolute Error: 58388.4658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "# Find the error rate on the training set\n",
    "mse = mean_absolute_error(y_train, model.predict(X_train))\n",
    "print(\"Training Set Mean Absolute Error: %.4f\" % mse)\n",
    "\n",
    "# Find the error rate on the test set\n",
    "mse = mean_absolute_error(y_test, model.predict(X_test))\n",
    "print(\"Test Set Mean Absolute Error: %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The defirence between these two numbers tells us alot about how well the model is working. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Our System:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can indirecty tell if our model is overfitting or underfitting based on the training set and test set error rates.  \n",
    "\n",
    "**Overfitting**: The model perfectly fit our training data, but didn't generalize to fit the test data at all. \n",
    "* Training set error very low\n",
    "* Test set error very high\n",
    "\n",
    "To solve this problem we should make our model less comolex by applying the following solutions:\n",
    "![](OverFitting.png)\n",
    "\n",
    " **Overfitting**: The model couldn't capture the patterns in the data set very well \n",
    "* Training set error very high\n",
    "* Test set error very high\n",
    "\n",
    "To solve this problem we can make our model more comolex by applying the following solutions:\n",
    "![](UnderFitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix those Problems we should adapt the hyper-parameters. The Machine Learning Models have lots of hyper-parameter to adjust. The best method to tune them is through trial and error.\n",
    "\n",
    "The first step is to declare our model without passing in any parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create the model\n",
    "model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the parameter grid. A good strategy is to try a few values for each parameter, where it increases or decreases by a signifcant amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters we want to try\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 1000, 3000],\n",
    "    'max_depth': [4, 6],\n",
    "    'min_samples_leaf': [3, 5, 9, 17],\n",
    "    'learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
    "    'max_features': [1.0, 0.3, 0.1],\n",
    "    'loss': ['ls', 'lad', 'huber']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next define the grid search using the GreadSearchCV function. CV stands for cross-validation. This funcion will automatically slice up the training data into smaller subsets and use part of the data for trainig different models and a different part of the data for testing those models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the grid search we want to run. Run it with four cpus in parallel.\n",
    "gs_cv = GridSearchCV(model, param_grid, n_jobs=4)\n",
    "\n",
    "# Run the grid search - on only the training data!\n",
    "gs_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the parameters that gave us the best result!\n",
    "print(gs_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the error rate on the training set using the best parameters\n",
    "mse = mean_absolute_error(y_train, gs_cv.predict(X_train))\n",
    "print(\"Training Set Mean Absolute Error: %.4f\" % mse)\n",
    "\n",
    "# Find the error rate on the test set using the best parameters\n",
    "mse = mean_absolute_error(y_test, gs_cv.predict(X_test))\n",
    "print(\"Test Set Mean Absolute Error: %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Selection:\n",
    "There are many features that are probably really importent to deteminig the value of tha house for example the size of the house. Other features, probably matter less. With the tree based machine learning alghorithm like gradinet boosting, we can look at the train modle and have it tell us how often each feature is used in detemining the final price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city_Martinezfort - 0.00%\n",
      "city_Julieberg - 0.00%\n",
      "city_New Michele - 0.00%\n",
      "city_New Robinton - 0.00%\n",
      "city_Davidtown - 0.02%\n",
      "city_Rickytown - 0.07%\n",
      "city_West Terrence - 0.08%\n",
      "city_West Brittanyview - 0.10%\n",
      "city_Amystad - 0.11%\n",
      "city_Leahview - 0.12%\n",
      "city_Fosterberg - 0.12%\n",
      "city_Lake Jennifer - 0.14%\n",
      "city_Clarkberg - 0.15%\n",
      "city_Port Daniel - 0.16%\n",
      "city_Toddshire - 0.16%\n",
      "city_South Stevenfurt - 0.17%\n",
      "city_West Lydia - 0.18%\n",
      "city_Brownport - 0.18%\n",
      "city_Joshuafurt - 0.19%\n",
      "city_Port Adamtown - 0.19%\n",
      "city_East Justin - 0.20%\n",
      "city_West Gerald - 0.21%\n",
      "city_Davidfort - 0.22%\n",
      "city_Lake Carolyn - 0.23%\n",
      "city_Jenniferberg - 0.24%\n",
      "city_East Lucas - 0.26%\n",
      "city_Wendybury - 0.27%\n",
      "city_Lake Christinaport - 0.30%\n",
      "city_East Janiceville - 0.31%\n",
      "city_Hallfort - 0.32%\n",
      "city_Justinport - 0.33%\n",
      "city_Morrisport - 0.33%\n",
      "city_West Gregoryview - 0.34%\n",
      "city_Port Jonathanborough - 0.36%\n",
      "city_East Amychester - 0.36%\n",
      "city_Scottberg - 0.37%\n",
      "city_Lake Dariusborough - 0.40%\n",
      "city_Richardport - 0.42%\n",
      "city_South Anthony - 0.51%\n",
      "city_Jeffreyhaven - 0.54%\n",
      "city_North Erinville - 0.55%\n",
      "city_Lewishaven - 0.62%\n",
      "has_central_heating - 0.64%\n",
      "city_Port Andrealand - 0.66%\n",
      "city_Chadstad - 0.73%\n",
      "has_central_cooling - 0.82%\n",
      "city_West Ann - 0.91%\n",
      "garage_type_detached - 0.92%\n",
      "city_Coletown - 0.98%\n",
      "garage_type_none - 1.06%\n",
      "garage_type_attached - 1.10%\n",
      "city_Lake Jack - 1.36%\n",
      "has_pool - 1.74%\n",
      "has_fireplace - 1.95%\n",
      "half_bathrooms - 2.01%\n",
      "stories - 2.31%\n",
      "full_bathrooms - 3.83%\n",
      "carport_sqft - 4.33%\n",
      "num_bedrooms - 4.55%\n",
      "garage_sqft - 13.42%\n",
      "year_built - 13.87%\n",
      "livable_sqft - 16.42%\n",
      "total_sqft - 16.54%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# These are the feature labels from our data set\n",
    "feature_labels = np.array(['year_built', 'stories', 'num_bedrooms', 'full_bathrooms', \n",
    "                           'half_bathrooms', 'livable_sqft', 'total_sqft', 'garage_sqft',\n",
    "                           'carport_sqft', 'has_fireplace', 'has_pool', 'has_central_heating',\n",
    "                           'has_central_cooling', 'garage_type_attached', 'garage_type_detached',\n",
    "                           'garage_type_none', 'city_Amystad', 'city_Brownport', 'city_Chadstad',\n",
    "                           'city_Clarkberg', 'city_Coletown', 'city_Davidfort', 'city_Davidtown',\n",
    "                           'city_East Amychester', 'city_East Janiceville', 'city_East Justin',\n",
    "                           'city_East Lucas', 'city_Fosterberg', 'city_Hallfort', 'city_Jeffreyhaven',\n",
    "                           'city_Jenniferberg', 'city_Joshuafurt', 'city_Julieberg', 'city_Justinport',\n",
    "                           'city_Lake Carolyn', 'city_Lake Christinaport', 'city_Lake Dariusborough',\n",
    "                           'city_Lake Jack', 'city_Lake Jennifer', 'city_Leahview', 'city_Lewishaven',\n",
    "                           'city_Martinezfort', 'city_Morrisport', 'city_New Michele', 'city_New Robinton',\n",
    "                           'city_North Erinville', 'city_Port Adamtown', 'city_Port Andrealand', \n",
    "                           'city_Port Daniel', 'city_Port Jonathanborough', 'city_Richardport',\n",
    "                           'city_Rickytown', 'city_Scottberg', 'city_South Anthony', 'city_South Stevenfurt',\n",
    "                           'city_Toddshire', 'city_Wendybury', 'city_West Ann', 'city_West Brittanyview',\n",
    "                           'city_West Gerald', 'city_West Gregoryview', 'city_West Lydia', 'city_West Terrence'])\n",
    "\n",
    "# Load the trained model created with train_model.py\n",
    "model = joblib.load('trained_house_classifier_model.pkl')\n",
    "\n",
    "# Create a numpy array based on the model's feature importances\n",
    "importance = model.feature_importances_\n",
    "\n",
    "# Sort the feature labels based on the feature importance rankings from the model\n",
    "feauture_indexes_by_importance = importance.argsort()\n",
    "\n",
    "# Print each feature label, from most important to least important (reverse order)\n",
    "for index in feauture_indexes_by_importance:\n",
    "    print(\"{} - {:.2f}%\".format(feature_labels[index], (importance[index] * 100.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Estimator in a Real-World Program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This house has an estimated value of $552,706.28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# Load the model we trained previously\n",
    "model = joblib.load('trained_house_classifier_model.pkl')\n",
    "\n",
    "# For the house we want to value, we need to provide the features in the exact same\n",
    "# arrangement as our training data set.\n",
    "house_to_value = [\n",
    "    # House features\n",
    "    2006,   # year_built\n",
    "    1,      # stories\n",
    "    4,      # num_bedrooms\n",
    "    3,      # full_bathrooms\n",
    "    0,      # half_bathrooms \n",
    "    2200,   # livable_sqft\n",
    "    2350,   # total_sqft\n",
    "    0,      # garage_sqft\n",
    "    0,      # carport_sqft\n",
    "    True,   # has_fireplace\n",
    "    False,  # has_pool\n",
    "    True,   # has_central_heating\n",
    "    True,   # has_central_cooling\n",
    "    \n",
    "    # Garage type: Choose only one\n",
    "    0,      # attached\n",
    "    0,      # detached\n",
    "    1,      # none\n",
    "    \n",
    "    # City: Choose only one\n",
    "    0,      # Amystad\n",
    "    1,      # Brownport\n",
    "    0,      # Chadstad\n",
    "    0,      # Clarkberg\n",
    "    0,      # Coletown\n",
    "    0,      # Davidfort\n",
    "    0,      # Davidtown\n",
    "    0,      # East Amychester\n",
    "    0,      # East Janiceville\n",
    "    0,      # East Justin\n",
    "    0,      # East Lucas\n",
    "    0,      # Fosterberg\n",
    "    0,      # Hallfort\n",
    "    0,      # Jeffreyhaven\n",
    "    0,      # Jenniferberg\n",
    "    0,      # Joshuafurt\n",
    "    0,      # Julieberg\n",
    "    0,      # Justinport\n",
    "    0,      # Lake Carolyn\n",
    "    0,      # Lake Christinaport\n",
    "    0,      # Lake Dariusborough\n",
    "    0,      # Lake Jack\n",
    "    0,      # Lake Jennifer\n",
    "    0,      # Leahview\n",
    "    0,      # Lewishaven\n",
    "    0,      # Martinezfort\n",
    "    0,      # Morrisport\n",
    "    0,      # New Michele\n",
    "    0,      # New Robinton\n",
    "    0,      # North Erinville\n",
    "    0,      # Port Adamtown\n",
    "    0,      # Port Andrealand\n",
    "    0,      # Port Daniel\n",
    "    0,      # Port Jonathanborough\n",
    "    0,      # Richardport\n",
    "    0,      # Rickytown\n",
    "    0,      # Scottberg\n",
    "    0,      # South Anthony\n",
    "    0,      # South Stevenfurt\n",
    "    0,      # Toddshire\n",
    "    0,      # Wendybury\n",
    "    0,      # West Ann\n",
    "    0,      # West Brittanyview\n",
    "    0,      # West Gerald\n",
    "    0,      # West Gregoryview\n",
    "    0,      # West Lydia\n",
    "    0       # West Terrence\n",
    "]\n",
    "\n",
    "# scikit-learn assumes you want to predict the values for lots of houses at once, so it expects an array.\n",
    "# We just want to look at a single house, so it will be the only item in our array.\n",
    "homes_to_value = [\n",
    "    house_to_value\n",
    "]\n",
    "\n",
    "# Run the model and make a prediction for each house in the homes_to_value array\n",
    "predicted_home_values = model.predict(homes_to_value)\n",
    "\n",
    "# Since we are only predicting the price of one house, just look at the first prediction returned\n",
    "predicted_value = predicted_home_values[0]\n",
    "\n",
    "print(\"This house has an estimated value of ${:,.2f}\".format(predicted_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how are the other features importent we can change one or two of them and predict the price of the house if the chnage in the price was significant then we can say this feature is so importent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
